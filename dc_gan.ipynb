{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORT LIBRARIES\n",
        "\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # HELPER FUNCTION FOR EVALUATION\n",
        "\n",
        "# def display_model_prediction(prediction_model, latent_dim, id=0):\n",
        "#     # compare false positive and false negative rates\n",
        "#     # true_lpoints = generate_latent_points_from_images([id], ext_format=lambda x:f\"{x}.jpg\", folder_name=\"mytests\", latent_dim=latent_dim)\n",
        "#     false_lpoints = generate_random_latent_points(latent_dim, 1)\n",
        "    \n",
        "#     # prediction_t = np.asarray((prediction_model.predict(true_lpoints)*127.5)+127.5).reshape(png_shape)\n",
        "#     prediction_f = np.asarray((prediction_model.predict(false_lpoints)*127.5)+127.5).reshape(png_shape)\n",
        "    \n",
        "#     # true_points = np.array(Image.open(f\"images/{random.randint(0,1024):04}.png\")).reshape(expanded_png_shape)\n",
        "#     # probability_t = discriminator.predict(prediction_t.reshape(expanded_png_shape))\n",
        "#     probability_f = discriminator.predict(prediction_f.reshape(expanded_png_shape))\n",
        "#     print(probability_f)\n",
        "\n",
        "#     # display and return true image\n",
        "#     image = tf.keras.utils.array_to_img(prediction_f)\n",
        "#     return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING DATA SETUP\n",
        "\n",
        "def load_real_data(folder_name=\"afremov\", dataset_sz=10, ext_format=lambda x:f\"{x}.jpg\", base_sz=128):\n",
        "    all_images = []\n",
        "    for i in range(dataset_sz):\n",
        "        # load image\n",
        "        img = Image.open(f\"data/afremov/{folder_name}/{ext_format(i)}\")\n",
        "\n",
        "        # scale training image to dimension of testing image\n",
        "        img = img.resize((base_sz, base_sz))\n",
        "\n",
        "        # add to dataset\n",
        "        img_data = tf.keras.utils.img_to_array(img, dtype=\"uint8\")\n",
        "        img_data = (img_data - 127.5) / 127.5\n",
        "        all_images.append(img_data)\n",
        "    result = tf.convert_to_tensor(all_images)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOISE GENERATION\n",
        "\n",
        "def generate_random_latent_points(latent_dim, n):\n",
        "    # generate a random array of size latent_dim (compressed 24x24)\n",
        "    x_input = np.random.randn(latent_dim * n)\n",
        "    x_input = x_input.reshape((n, latent_dim))\n",
        "    return x_input   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING SAMPLES\n",
        "\n",
        "def generate_real_samples(dataset, n):\n",
        "    indices = random.sample(range(len(dataset)), n)\n",
        "    new_x = tf.gather(dataset, indices=indices)\n",
        "    new_y = tf.ones((n, 1))\n",
        "    return new_x, new_y\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "    x_input = generate_random_latent_points(latent_dim, n)\n",
        "    new_x = generator.predict(x_input)\n",
        "    new_y = tf.zeros((n, 1))\n",
        "    return new_x, new_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDEHxoijoVw7"
      },
      "outputs": [],
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Reshape\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import L2\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "# MODEL DEFINITION\n",
        "\n",
        "def discriminator_model(in_shape):\n",
        "  model = Sequential()\n",
        "  num_features = in_shape[0]\n",
        "\n",
        "  # convolution\n",
        "  model.add(Conv2D(filters=num_features,\n",
        "                   kernel_size=3,\n",
        "                   strides=1,\n",
        "                   padding='same',\n",
        "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                   input_shape=in_shape,\n",
        "                   use_bias=True))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(2,2))\n",
        "    \n",
        "  # convolution again\n",
        "  model.add(Conv2D(filters=num_features,\n",
        "                   kernel_size=3,\n",
        "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                   padding='same',\n",
        "                   use_bias=True))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(2,2))\n",
        "\n",
        "  # convolution one more\n",
        "  model.add(Conv2D(filters=num_features*2,\n",
        "                   kernel_size=3,\n",
        "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                   kernel_regularizer=L2(l=0.0001),\n",
        "                   padding='same',\n",
        "                   use_bias=True))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(2,2))\n",
        "\n",
        "  # classifier\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(num_features*4,activation='relu', use_bias=True, kernel_regularizer=L2(l=0.0001)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(3, activation='softmax', use_bias=True))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def generator_model(pic_dim, num_channels, latent_dim):\n",
        "  model = Sequential()\n",
        "  num_features = pic_dim\n",
        "  n_nodes = (pic_dim**2) * num_features\n",
        "\n",
        "  # transform latent points to small image (8x8)\n",
        "  model.add(Dense(n_nodes,\n",
        "                  input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((pic_dim, pic_dim, num_features)))\n",
        "\n",
        "  # upsample to 16x16\n",
        "  model.add(Conv2DTranspose(filters=num_features,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # # upsample to 32x32\n",
        "  model.add(Conv2DTranspose(filters=num_features,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                            kernel_regularizer=L2(l=0.0001),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # upsample to 64x64\n",
        "  model.add(Conv2DTranspose(filters=num_features,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                            kernel_regularizer=L2(l=0.0001),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # upsample to 128x128\n",
        "  model.add(Conv2DTranspose(filters=num_features,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                            kernel_regularizer=L2(l=0.0001),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # finish with tanh activation for each channel\n",
        "  model.add(Conv2D(filters=num_channels,\n",
        "                   kernel_size=8,\n",
        "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
        "                   activation='tanh',\n",
        "                   padding='same'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def define_gan(gen, disc):\n",
        "  # lock weights for the discriminator\n",
        "  disc.trainable = False\n",
        "\n",
        "  # create joint network\n",
        "  model = Sequential()\n",
        "  model.add(gen)\n",
        "  model.add(disc)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN MODELS\n",
        "\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, epochs=100, n_batch=32):\n",
        "    batch_per_epoch = dataset.shape[0]//n_batch\n",
        "    if batch_per_epoch == 0:\n",
        "        raise AssertionError(\"n_batch must be smaller than dataset size\")\n",
        "        \n",
        "    for i in range(epochs):\n",
        "        for _ in range(batch_per_epoch):\n",
        "            # train discriminator to recognize real samples\n",
        "            x_real, y_real = generate_real_samples(dataset, n_batch)\n",
        "            x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
        "            x_total, y_total = shuffle(np.concatenate([x_real, x_fake]), np.concatenate([y_real, y_fake]))\n",
        "            d_model.trainable = True\n",
        "            d_model.train_on_batch(x_total, y_total)\n",
        "\n",
        "            # train generator to fool discriminator\n",
        "            x_gan = generate_random_latent_points(latent_dim=latent_dim, n=n_batch)\n",
        "            y_gan = tf.ones((n_batch, 1)) # the \"false\" one\n",
        "            d_model.trainable = False\n",
        "            gan_model.train_on_batch(x_gan, y_gan)\n",
        "            \n",
        "            \n",
        "        if i%(epochs/100) == (epochs/100)-1:\n",
        "            prediction = np.asarray((g_model.predict(generate_random_latent_points(latent_dim, 1))*127.5)+127.5).reshape(png_shape)\n",
        "            img = tf.keras.utils.array_to_img(prediction)\n",
        "            img.save(f\"data/dcgan_result/iter_{i+1}.png\")\n",
        "            print(f\"{int((i+1)/epochs * 100)}% complete\")\n",
        "    return g_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CREATE MODELS\n",
        "\n",
        "# setup\n",
        "latent_dim = 1024\n",
        "num_channels = 3\n",
        "pix_width = 128\n",
        "low_res = 8\n",
        "png_shape = (pix_width, pix_width, num_channels)\n",
        "expanded_png_shape = (1, pix_width, pix_width, num_channels)\n",
        "\n",
        "# models\n",
        "discriminator = discriminator_model(png_shape)\n",
        "# print(discriminator.summary())\n",
        "generator = generator_model(low_res, num_channels, latent_dim)\n",
        "# print(generator.summary())\n",
        "gan_model = define_gan(generator, discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train\n",
        "dataset = load_real_data(dataset_sz=60)\n",
        "model = train(generator, discriminator, gan_model, dataset, latent_dim, epochs=1000, n_batch=32)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base-tf-keras-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1aeeb17421a8e1201a0fe1bd1f3d7531481ce183aa5b238c681e888efd505899"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
