{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDEHxoijoVw7"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# MODEL DEFINITION\n",
        "\n",
        "def discriminator_model(in_shape=(24,24,4)):\n",
        "  model = Sequential()\n",
        "    \n",
        "  # convolution\n",
        "  model.add(Conv2D(filters=128,\n",
        "                   kernel_size=(2,2),\n",
        "                   padding='same',\n",
        "                   data_format='channels_last',\n",
        "                   input_shape=in_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # convolution again\n",
        "  model.add(Conv2D(filters=128,\n",
        "                   kernel_size=(2,2),\n",
        "                   padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  # classifier\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def generator_model(latent_dim=100, pic_dim=6, num_channels=4):\n",
        "  model = Sequential()\n",
        "  num_copies = 128\n",
        "  n_nodes = pic_dim**2 * num_copies\n",
        "\n",
        "  # transform latent points to 6x6 image\n",
        "  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((pic_dim, pic_dim, num_copies)))\n",
        "\n",
        "  # upsample to 12x12\n",
        "  model.add(Conv2DTranspose(filters=num_copies,\n",
        "                            kernel_size=(4,4),\n",
        "                            strides=(2,2),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # upsample to 24x24\n",
        "  model.add(Conv2DTranspose(filters=num_copies,\n",
        "                            kernel_size=(4,4),\n",
        "                            strides=(2,2),\n",
        "                            padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # finish with tanh activation\n",
        "  model.add(Conv2D(filters=num_channels,\n",
        "                   kernel_size=(3,3),\n",
        "                   activation='tanh',\n",
        "                   padding='same'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def define_gan(generator, discriminator):\n",
        "  # lock weights for the discriminator\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  # create joint network\n",
        "  model = Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from tensorflow import convert_to_tensor\n",
        "from tensorflow import expand_dims\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# TRAINING DATA SETUP\n",
        "\n",
        "def retrieve_data(num_points=1000):\n",
        "    # setup\n",
        "    folder_name = \"images\"\n",
        "    image_url = lambda id : f\"https://cryptopunks.app/public/images/cryptopunks/punk{id:04}.png\"\n",
        "    try:\n",
        "        os.mkdir(folder_name)\n",
        "    except: pass\n",
        "\n",
        "    # request pngs\n",
        "    for i in range(num_points):\n",
        "        img_data = requests.get(image_url(i), stream=True)\n",
        "        with open(f\"{folder_name}/{i:04}.png\", \"wb\") as f:\n",
        "            shutil.copyfileobj(img_data.raw, f)\n",
        "\n",
        "def load_real_data(folder_name, dataset_sz=10):\n",
        "    all_images = []\n",
        "    for i in range(dataset_sz):\n",
        "        img_data = convert_to_tensor(Image.open(f\"{folder_name}/{i:04}.png\"), dtype='float32')\n",
        "        img_data = (img_data - 127.5) / 127.5\n",
        "        all_images.append(img_data)\n",
        "    return convert_to_tensor(all_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.random import randn\n",
        "\n",
        "# NOISE GENERATION\n",
        "\n",
        "def generate_random_latent_points(latent_dim, n):\n",
        "    # generate a random array of size latent_dim (compressed 24x24)\n",
        "    x_input = randn(latent_dim * n)\n",
        "    x_input = x_input.reshape((n, latent_dim))\n",
        "    return x_input\n",
        "\n",
        "def generate_latent_points_from_image(id=0):\n",
        "    # encode image to the latent space\n",
        "    pass\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from tensorflow import gather\n",
        "from tensorflow import ones\n",
        "from tensorflow import zeros\n",
        "\n",
        "# TRAINING SAMPLES\n",
        "\n",
        "def generate_real_samples(dataset, n):\n",
        "    sample = random.sample(range(len(dataset)), n)\n",
        "    new_x = gather(dataset, indices=sample)\n",
        "    new_y = ones((n, 1))\n",
        "    return new_x, new_y\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "    x_input = generate_random_latent_points(latent_dim, n)\n",
        "    new_x = generator.predict(x_input)\n",
        "    new_y = zeros((n, 1))\n",
        "    return new_x, new_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN MODELS\n",
        "\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, num_channels, epochs=100, n_batch=32):\n",
        "    batch_per_epoch = dataset.shape[0]//n_batch\n",
        "    half_batch = n_batch//2\n",
        "    for i in range(epochs):\n",
        "        for j in range(batch_per_epoch):\n",
        "            x_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            d_model.train_on_batch(x_real, y_real)\n",
        "            x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            d_model.train_on_batch(x_fake, y_fake)\n",
        "            x_gan = generate_random_latent_points(latent_dim, n_batch)\n",
        "            y_gan = ones((n_batch, 1))\n",
        "            g_loss = gan_model.train_on_batch(x_gan, y_gan)\n",
        "            \n",
        "        if i%(epochs/10) == (epochs/10)-1:\n",
        "            print(f\"{int((i+1)/epochs * 100)}% complete\")\n",
        "    return g_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# CREATE AND TRAIN MODELS\n",
        "\n",
        "latent_dim = 100\n",
        "num_channels = 4\n",
        "discriminator = discriminator_model()\n",
        "generator = generator_model(latent_dim, 6, num_channels)\n",
        "\n",
        "# train the model\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "dataset = load_real_data(\"images\", 64)\n",
        "model = train(generator, discriminator, gan_model, dataset, latent_dim, num_channels, epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import expand_dims\n",
        "# predict from input\n",
        "latent_points = generate_random_latent_points(100, 1)\n",
        "prediction = np.array(generator.predict(latent_points)).reshape(24,24,4)\n",
        "image = Image.fromarray(prediction, mode=\"RGBA\")\n",
        "# print(prediction)\n",
        "display(image.resize((400,400)))\n",
        "\n",
        "# false_points = np.random.randn(24*24*4).reshape((1,24,24,4))\n",
        "# true_points = np.array(Image.open(f\"images/{2:04}.png\")).reshape((1,24,24,4))\n",
        "# prediction_f = discriminator.predict(false_points)\n",
        "# prediction_t = discriminator.predict(true_points)\n",
        "# print(prediction_f, prediction_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "201ba35bebf75ca7ae57d95fa9e77373043099ac93c77c41fc6dd4a4b62d7c8c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
